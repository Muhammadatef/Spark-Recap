{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1a4261b-a39c-49f0-a113-0067ebeab3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd91f0f-0c18-4285-b183-1435f7f73e6b",
   "metadata": {},
   "source": [
    "### Inspired by pandas DataFrames in structure, format, and a few specific operations,\n",
    "#### Spark DataFrames are like distributed in-memory tables with named columns and schemas, where each column has a specific data type: integer, string, array, map, real,date, timestamp, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e76555d-8dc5-4879-bfe5-dd51dcfe94ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0fdd28-9206-440a-9911-e85b06e8d86c",
   "metadata": {},
   "source": [
    "### Dealing with missing data with pyspark\n",
    "#### Missing Data\n",
    "1. Keep them.\n",
    "2. Remove them.\n",
    "3. Fill them with some values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a93ccff0-b5b3-4338-8328-cf025de7d6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('NullData.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8773b714-d6eb-465b-96a6-7ee7ebaa82df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John| NULL|\n",
      "|emp2| NULL| NULL|\n",
      "|emp3| NULL|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f53e4bd-a135-443a-8446-6aa4f2d50c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: string (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sales: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c06aa5db-f5cf-4fa6-b331-35eb3701b0fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## How to deal with Missin Values in Spark?\n",
    "df.na.drop().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18174509-e7a8-4462-bb29-9c08f5fa6793",
   "metadata": {},
   "source": [
    "#### If specified, drop rows that have less than `thresh` non-null values.\n",
    "#### This overwrites the `how` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e6e5c82-6a9e-451a-9ee3-279361eff85f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John| NULL|\n",
      "|emp2| NULL| NULL|\n",
      "|emp3| NULL|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df.na.drop(thresh=1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "064bc5f7-77b6-4b61-82a8-9b2dab45f807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John| NULL|\n",
      "|emp3| NULL|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## thresh=2 means that the row that has at least two values in it\n",
    "df.na.drop(thresh=2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff5020f6-3df3-4e73-aaf0-3c37b0819201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## thresh=3 means that the row that has at least 3 values in it\n",
    "\n",
    "df.na.drop(thresh=3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9558af58-90e5-4e48-a3e9-a7ce8c930e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp3| NULL|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop(subset=['Sales']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eedb839c-f0a1-4659-93ae-78250ff1f8ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop(subset=['Name','Sales'], thresh=2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f336cd1-3465-4118-adfb-b9a0c0deed2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+-----+\n",
      "|  Id|   Name|Sales|\n",
      "+----+-------+-----+\n",
      "|emp1|   John| NULL|\n",
      "|emp2|No Name| NULL|\n",
      "|emp3|No Name|345.0|\n",
      "|emp4|  Cindy|456.0|\n",
      "+----+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## how to fill the null values in Spark?\n",
    "\n",
    "df.na.fill('No Name').show()\n",
    "## it went to the string column that has nulls automatically by spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c65501d6-a7da-4e43-b993-dc3e15ec550b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John| 25.0|\n",
      "|emp2| NULL| 25.0|\n",
      "|emp3| NULL|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.fill(25).show()\n",
    "## it went to the integer column that has nulls automatically by spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bbe24e61-87f8-49ad-a80c-e7ea347ed1bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John| 25.0|\n",
      "|emp2| NULL| 25.0|\n",
      "|emp3| NULL|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.fill(25, subset=['Sales']).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15de06d3-e837-41ed-b94f-8bb88972e15a",
   "metadata": {},
   "source": [
    "### DataFrame Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368f66ca-0ffa-4da4-a060-648c2d8e2637",
   "metadata": {},
   "source": [
    "In Python, it’s possible to access a DataFrame’s columns either by attribute <b>(df.age)</b> or by indexing <b>(df['age'])</b>. While the former is convenient for interactive data exploration, users are highly encouraged to use the latter form, which is future proof and won’t break with column names that are also attributes on the DataFrame class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bfe322f1-71a2-42d8-9e9b-04db22d9576a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean \n",
    "mean_value = df.select(mean(df['Sales']).alias('SalesMean')).collect()[0].SalesMean\n",
    "\n",
    "\n",
    "##mean_value = df.select(mean(df['Sales']).alias('SalesMean')).collect()[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "104bfc93-1d3e-4a9d-99e6-def80b95c384",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400.5"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "67cc6ff0-9222-421f-9fc7-287c0c5edbca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John|400.5|\n",
      "|emp2| NULL|400.5|\n",
      "|emp3| NULL|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.fill(mean_value, subset= ['Sales']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "12ec2203-2586-480b-9462-78715af6f79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sometime in machine learning, you want to transform subset of Spark DF to Pandas DF \n",
    "##to make some feature engineering, here is how to do it \n",
    "df_toPandas = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "55479c0d-2ed3-4ee1-8d17-de43539163a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>emp1</td>\n",
       "      <td>John</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>emp2</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>emp3</td>\n",
       "      <td>None</td>\n",
       "      <td>345.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>emp4</td>\n",
       "      <td>Cindy</td>\n",
       "      <td>456.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Id   Name  Sales\n",
       "0  emp1   John    NaN\n",
       "1  emp2   None    NaN\n",
       "2  emp3   None  345.0\n",
       "3  emp4  Cindy  456.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_toPandas.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f46f57-3c86-48e5-b4a6-97cfbd7a9dfd",
   "metadata": {},
   "source": [
    "## Schemas and Creating DataFrames\n",
    "\n",
    "A schema in Spark defines the column names and associated data types for a DataFrame. Most often, schemas come into play when you are reading structured data\n",
    "from an external data source Defining a schema\n",
    "up front as opposed to taking a schema-on-read approach offers three benefits:\n",
    "<b>\n",
    "1. You relieve Spark from the onus of inferring data types.\n",
    "2. You prevent Spark from creating a separate job just to read a large portion of your file to ascertain the schema, which for a large data file can be expensive and time-consuming.\n",
    "3. You can detect errors early if data doesn’t match the schema.\n",
    "</b>\n",
    "\n",
    "<i>So, it is encouraged to always define your schema up front whenever you want to\n",
    "read a large file from a data source.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ffd540ce-33d4-41e4-8829-81f32726c7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fire = spark.read.csv('sf-fire-calls.csv', header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b0bf8639-6bf7-435d-8049-c92a73ddaf00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CallNumber: integer (nullable = true)\n",
      " |-- UnitID: string (nullable = true)\n",
      " |-- IncidentNumber: integer (nullable = true)\n",
      " |-- CallType: string (nullable = true)\n",
      " |-- CallDate: string (nullable = true)\n",
      " |-- WatchDate: string (nullable = true)\n",
      " |-- CallFinalDisposition: string (nullable = true)\n",
      " |-- AvailableDtTm: string (nullable = true)\n",
      " |-- Address: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Zipcode: integer (nullable = true)\n",
      " |-- Battalion: string (nullable = true)\n",
      " |-- StationArea: string (nullable = true)\n",
      " |-- Box: string (nullable = true)\n",
      " |-- OriginalPriority: string (nullable = true)\n",
      " |-- Priority: string (nullable = true)\n",
      " |-- FinalPriority: integer (nullable = true)\n",
      " |-- ALSUnit: boolean (nullable = true)\n",
      " |-- CallTypeGroup: string (nullable = true)\n",
      " |-- NumAlarms: integer (nullable = true)\n",
      " |-- UnitType: string (nullable = true)\n",
      " |-- UnitSequenceInCallDispatch: integer (nullable = true)\n",
      " |-- FirePreventionDistrict: string (nullable = true)\n",
      " |-- SupervisorDistrict: string (nullable = true)\n",
      " |-- Neighborhood: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- RowID: string (nullable = true)\n",
      " |-- Delay: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_fire.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cbaea0-0297-4d63-956e-d69fd596c185",
   "metadata": {},
   "source": [
    "#### we are working with BigData, to tell Spark to infer the schema with huge dataset\n",
    "#### it will go to read all the dataset and this may be too much load and processing for Spark\n",
    "#### to solve that, use samplingRatio=0.001 feature, to take only small sample to infer the schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d481cfb-1a2c-40ed-9a43-1fe5d1821c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_fire_sample = spark.read.csv('sf-fire-calls.csv', header=True,inferSchema=True, samplingRatio=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6fa4dabe-2922-4fee-8fd8-a52f4554b480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CallNumber: integer (nullable = true)\n",
      " |-- UnitID: string (nullable = true)\n",
      " |-- IncidentNumber: integer (nullable = true)\n",
      " |-- CallType: string (nullable = true)\n",
      " |-- CallDate: string (nullable = true)\n",
      " |-- WatchDate: string (nullable = true)\n",
      " |-- CallFinalDisposition: string (nullable = true)\n",
      " |-- AvailableDtTm: string (nullable = true)\n",
      " |-- Address: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Zipcode: integer (nullable = true)\n",
      " |-- Battalion: string (nullable = true)\n",
      " |-- StationArea: integer (nullable = true)\n",
      " |-- Box: integer (nullable = true)\n",
      " |-- OriginalPriority: string (nullable = true)\n",
      " |-- Priority: string (nullable = true)\n",
      " |-- FinalPriority: integer (nullable = true)\n",
      " |-- ALSUnit: boolean (nullable = true)\n",
      " |-- CallTypeGroup: string (nullable = true)\n",
      " |-- NumAlarms: integer (nullable = true)\n",
      " |-- UnitType: string (nullable = true)\n",
      " |-- UnitSequenceInCallDispatch: integer (nullable = true)\n",
      " |-- FirePreventionDistrict: string (nullable = true)\n",
      " |-- SupervisorDistrict: integer (nullable = true)\n",
      " |-- Neighborhood: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- RowID: string (nullable = true)\n",
      " |-- Delay: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_fire_sample.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bd9afe41-99c8-4b56-877a-50c40bd20785",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('CallNumber', IntegerType(), True), StructField('UnitID', StringType(), True), StructField('IncidentNumber', IntegerType(), True), StructField('CallType', StringType(), True), StructField('CallDate', StringType(), True), StructField('WatchDate', StringType(), True), StructField('CallFinalDisposition', StringType(), True), StructField('AvailableDtTm', StringType(), True), StructField('Address', StringType(), True), StructField('City', StringType(), True), StructField('Zipcode', IntegerType(), True), StructField('Battalion', StringType(), True), StructField('StationArea', IntegerType(), True), StructField('Box', IntegerType(), True), StructField('OriginalPriority', StringType(), True), StructField('Priority', StringType(), True), StructField('FinalPriority', IntegerType(), True), StructField('ALSUnit', BooleanType(), True), StructField('CallTypeGroup', StringType(), True), StructField('NumAlarms', IntegerType(), True), StructField('UnitType', StringType(), True), StructField('UnitSequenceInCallDispatch', IntegerType(), True), StructField('FirePreventionDistrict', StringType(), True), StructField('SupervisorDistrict', IntegerType(), True), StructField('Neighborhood', StringType(), True), StructField('Location', StringType(), True), StructField('RowID', StringType(), True), StructField('Delay', DoubleType(), True)])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fire_sample.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f010eb-db78-4eba-9cbb-f0952bbb7a59",
   "metadata": {},
   "source": [
    "#### Now you have the schema from the sample, you can take it and pass to the original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9154df50-97e6-4914-9d51-9717f79092d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fire = spark.read.csv('sf-fire-calls.csv', header=True,schema=df_fire_sample.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b8c414ae-9243-46a6-a3ca-cf7491ac9b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CallNumber: integer (nullable = true)\n",
      " |-- UnitID: string (nullable = true)\n",
      " |-- IncidentNumber: integer (nullable = true)\n",
      " |-- CallType: string (nullable = true)\n",
      " |-- CallDate: string (nullable = true)\n",
      " |-- WatchDate: string (nullable = true)\n",
      " |-- CallFinalDisposition: string (nullable = true)\n",
      " |-- AvailableDtTm: string (nullable = true)\n",
      " |-- Address: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Zipcode: integer (nullable = true)\n",
      " |-- Battalion: string (nullable = true)\n",
      " |-- StationArea: integer (nullable = true)\n",
      " |-- Box: integer (nullable = true)\n",
      " |-- OriginalPriority: string (nullable = true)\n",
      " |-- Priority: string (nullable = true)\n",
      " |-- FinalPriority: integer (nullable = true)\n",
      " |-- ALSUnit: boolean (nullable = true)\n",
      " |-- CallTypeGroup: string (nullable = true)\n",
      " |-- NumAlarms: integer (nullable = true)\n",
      " |-- UnitType: string (nullable = true)\n",
      " |-- UnitSequenceInCallDispatch: integer (nullable = true)\n",
      " |-- FirePreventionDistrict: string (nullable = true)\n",
      " |-- SupervisorDistrict: integer (nullable = true)\n",
      " |-- Neighborhood: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- RowID: string (nullable = true)\n",
      " |-- Delay: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_fire.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a3d0d148-a842-4057-9362-ff253c59baeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "import pyspark.sql.functions as fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a662e35a-be6a-4fb9-9b7f-41612a0ce86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Medical = df_fire.select('IncidentNumber', 'AvailableDtTm', 'CallType')\\\n",
    "        .where(col('CallType')=='Medical Incident') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6e5cfe43-3f0a-4e86-a8b1-ac5d3ce4ca51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------------+----------------+\n",
      "|IncidentNumber|AvailableDtTm         |CallType        |\n",
      "+--------------+----------------------+----------------+\n",
      "|2003241       |01/11/2002 03:01:18 AM|Medical Incident|\n",
      "|2003242       |01/11/2002 02:39:50 AM|Medical Incident|\n",
      "|2003343       |01/11/2002 12:06:57 PM|Medical Incident|\n",
      "|2003348       |01/11/2002 01:08:40 PM|Medical Incident|\n",
      "|2003381       |01/11/2002 03:31:02 PM|Medical Incident|\n",
      "+--------------+----------------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_Medical.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "254918a6-517f-4aaa-b055-3f9537de90d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_MedicalNotNull = df_fire.select('IncidentNumber', 'AvailableDtTm', 'CallType')\\\n",
    "        .where(col('CallType').isNotNull()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d9dbde12-50f7-4438-bcfe-0d06ea30c68d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+--------------------+\n",
      "|IncidentNumber|       AvailableDtTm|            CallType|\n",
      "+--------------+--------------------+--------------------+\n",
      "|       2003235|01/11/2002 01:51:...|      Structure Fire|\n",
      "|       2003241|01/11/2002 03:01:...|    Medical Incident|\n",
      "|       2003242|01/11/2002 02:39:...|    Medical Incident|\n",
      "|       2003250|01/11/2002 04:16:...|        Vehicle Fire|\n",
      "|       2003259|01/11/2002 06:01:...|              Alarms|\n",
      "|       2003279|01/11/2002 08:03:...|      Structure Fire|\n",
      "|       2003301|01/11/2002 09:46:...|              Alarms|\n",
      "|       2003304|01/11/2002 09:58:...|              Alarms|\n",
      "|       2003343|01/11/2002 12:06:...|    Medical Incident|\n",
      "|       2003348|01/11/2002 01:08:...|    Medical Incident|\n",
      "|       2003381|01/11/2002 03:31:...|    Medical Incident|\n",
      "|       2003382|01/11/2002 02:59:...|      Structure Fire|\n",
      "|       2003399|01/11/2002 04:22:...|    Medical Incident|\n",
      "|       2003403|01/11/2002 04:18:...|    Medical Incident|\n",
      "|       2003408|01/11/2002 04:09:...|      Structure Fire|\n",
      "|       2003408|01/11/2002 04:09:...|      Structure Fire|\n",
      "|       2003408|01/11/2002 04:09:...|      Structure Fire|\n",
      "|       2003409|01/11/2002 04:34:...|    Medical Incident|\n",
      "|       2003417|01/11/2002 04:51:...|    Medical Incident|\n",
      "|       2003417|01/11/2002 04:51:...|    Medical Incident|\n",
      "|       2003429|01/11/2002 05:17:...|Odor (Strange / U...|\n",
      "|       2003435|01/11/2002 05:46:...|    Medical Incident|\n",
      "|       2003453|01/11/2002 06:48:...|              Alarms|\n",
      "|       2003497|01/11/2002 09:03:...|      Structure Fire|\n",
      "|       2003500|01/11/2002 10:08:...|    Medical Incident|\n",
      "|       2003529|01/11/2002 10:56:...|    Medical Incident|\n",
      "|       2003550|01/12/2002 02:04:...|    Medical Incident|\n",
      "|       2003554|01/12/2002 01:56:...|      Structure Fire|\n",
      "|       2003576|01/12/2002 04:17:...|    Medical Incident|\n",
      "|       2003577|01/12/2002 04:23:...|    Medical Incident|\n",
      "+--------------+--------------------+--------------------+\n",
      "only showing top 30 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_MedicalNotNull.show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e36d7701-c44f-4e6c-a3dc-f36d65c6805c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_CallDistinct = df_fire.select('CallType')\\\n",
    "        .where(col('CallType').isNotNull()).distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "af2c4229-060b-491f-8bc4-db9840257677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            CallType|\n",
      "+--------------------+\n",
      "|Elevator / Escala...|\n",
      "|         Marine Fire|\n",
      "|  Aircraft Emergency|\n",
      "|      Administrative|\n",
      "|              Alarms|\n",
      "|Odor (Strange / U...|\n",
      "|Citizen Assist / ...|\n",
      "|              HazMat|\n",
      "|Watercraft in Dis...|\n",
      "|           Explosion|\n",
      "|           Oil Spill|\n",
      "|        Vehicle Fire|\n",
      "|  Suspicious Package|\n",
      "|Extrication / Ent...|\n",
      "|               Other|\n",
      "|        Outside Fire|\n",
      "|   Traffic Collision|\n",
      "|       Assist Police|\n",
      "|Gas Leak (Natural...|\n",
      "|        Water Rescue|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_CallDistinct.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "49392193-4bac-4615-89d0-36348269fb67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+-------------+------+\n",
      "|CallType              |City         |UnitID|\n",
      "+----------------------+-------------+------+\n",
      "|Watercraft in Distress|SF           |E35   |\n",
      "|Watercraft in Distress|SF           |RC1   |\n",
      "|Watercraft in Distress|SF           |E16   |\n",
      "|Watercraft in Distress|PR           |E34   |\n",
      "|Watercraft in Distress|SF           |T08   |\n",
      "|Watercraft in Distress|SF           |E02   |\n",
      "|Watercraft in Distress|SF           |E13   |\n",
      "|Watercraft in Distress|SF           |E28   |\n",
      "|Watercraft in Distress|San Francisco|E35   |\n",
      "|Watercraft in Distress|SF           |FB1   |\n",
      "|Watercraft in Distress|SF           |RB1   |\n",
      "|Watercraft in Distress|TI           |B03   |\n",
      "|Watercraft in Distress|SAN FRANCISCO|B08   |\n",
      "|Watercraft in Distress|FM           |94    |\n",
      "|Watercraft in Distress|San Francisco|B10   |\n",
      "|Watercraft in Distress|San Francisco|RB1   |\n",
      "|Watercraft in Distress|San Francisco|RA48  |\n",
      "|Watercraft in Distress|San Francisco|RS2   |\n",
      "|Watercraft in Distress|SF           |RS1   |\n",
      "|Watercraft in Distress|SF           |RS2   |\n",
      "|Watercraft in Distress|SF           |B01   |\n",
      "|Watercraft in Distress|San Francisco|FB3   |\n",
      "|Water Rescue          |PR           |B11   |\n",
      "|Water Rescue          |SF           |T16   |\n",
      "|Water Rescue          |SF           |E16   |\n",
      "|Water Rescue          |SF           |CR1   |\n",
      "|Water Rescue          |SF           |B08   |\n",
      "|Water Rescue          |PR           |B04   |\n",
      "|Water Rescue          |SF           |E35   |\n",
      "|Water Rescue          |SF           |M08   |\n",
      "|Water Rescue          |PR           |E53   |\n",
      "|Water Rescue          |SF           |E51   |\n",
      "|Water Rescue          |SF           |E13   |\n",
      "|Water Rescue          |SF           |B09   |\n",
      "|Water Rescue          |SF           |B01   |\n",
      "|Water Rescue          |SF           |RS2   |\n",
      "|Water Rescue          |SF           |85    |\n",
      "|Water Rescue          |SF           |M28   |\n",
      "|Water Rescue          |PR           |E14   |\n",
      "|Water Rescue          |SF           |E34   |\n",
      "|Water Rescue          |SF           |M22   |\n",
      "|Water Rescue          |SF           |M43   |\n",
      "|Water Rescue          |SF           |96    |\n",
      "|Water Rescue          |SF           |B07   |\n",
      "|Water Rescue          |SF           |E19   |\n",
      "|Water Rescue          |PR           |E35   |\n",
      "|Water Rescue          |PR           |E18   |\n",
      "|Water Rescue          |SF           |E18   |\n",
      "|Water Rescue          |SF           |66    |\n",
      "|Water Rescue          |SF           |T18   |\n",
      "+----------------------+-------------+------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_fire.select('CallType','City','UnitID').where(col('CallType').isNotNull()) \\\n",
    "        .distinct() \\\n",
    "        .sort('CallType', ascending=False) \\\n",
    "        .show(50,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86127504-46db-4ec7-96f6-d64791aab8a3",
   "metadata": {},
   "source": [
    "#### Important Operations for Spark DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b119e585-5782-4893-a3e9-e331f135b2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fire2 = df_fire.withColumn('Delay in Seconds', col('Delay') * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4456c6d3-d3f9-4772-b5cc-035bcfa114fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CallNumber: integer (nullable = true)\n",
      " |-- UnitID: string (nullable = true)\n",
      " |-- IncidentNumber: integer (nullable = true)\n",
      " |-- CallType: string (nullable = true)\n",
      " |-- CallDate: string (nullable = true)\n",
      " |-- WatchDate: string (nullable = true)\n",
      " |-- CallFinalDisposition: string (nullable = true)\n",
      " |-- AvailableDtTm: string (nullable = true)\n",
      " |-- Address: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Zipcode: integer (nullable = true)\n",
      " |-- Battalion: string (nullable = true)\n",
      " |-- StationArea: integer (nullable = true)\n",
      " |-- Box: integer (nullable = true)\n",
      " |-- OriginalPriority: string (nullable = true)\n",
      " |-- Priority: string (nullable = true)\n",
      " |-- FinalPriority: integer (nullable = true)\n",
      " |-- ALSUnit: boolean (nullable = true)\n",
      " |-- CallTypeGroup: string (nullable = true)\n",
      " |-- NumAlarms: integer (nullable = true)\n",
      " |-- UnitType: string (nullable = true)\n",
      " |-- UnitSequenceInCallDispatch: integer (nullable = true)\n",
      " |-- FirePreventionDistrict: string (nullable = true)\n",
      " |-- SupervisorDistrict: integer (nullable = true)\n",
      " |-- Neighborhood: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- RowID: string (nullable = true)\n",
      " |-- Delay: double (nullable = true)\n",
      " |-- Delay in Seconds: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_fire2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "35a9b1ee-faae-44d7-b6cb-76e992949f9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[CallNumber: int, UnitID: string, IncidentNumber: int, CallType: string, CallDate: string, WatchDate: string, CallFinalDisposition: string, AvailableDtTm: string, Address: string, City: string, Zipcode: int, Battalion: string, StationArea: int, Box: int, OriginalPriority: string, Priority: string, FinalPriority: int, ALSUnit: boolean, CallTypeGroup: string, NumAlarms: int, UnitType: string, UnitSequenceInCallDispatch: int, FirePreventionDistrict: string, SupervisorDistrict: int, Neighborhood: string, Location: string, RowID: string, DelayInMins: double, Delay in Seconds: double]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fire2.withColumnRenamed('Delay', 'DelayInMins')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b4c1e268-45f8-4a31-b415-55735ab62d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fire_dt = df_fire.withColumn('IncidentDate',to_timestamp(col('CallDate'), 'MM/dd/yyyy')).drop('CallDate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "72b2574b-672f-4f07-a3cd-af2dbe2bf4e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CallNumber: integer (nullable = true)\n",
      " |-- UnitID: string (nullable = true)\n",
      " |-- IncidentNumber: integer (nullable = true)\n",
      " |-- CallType: string (nullable = true)\n",
      " |-- WatchDate: string (nullable = true)\n",
      " |-- CallFinalDisposition: string (nullable = true)\n",
      " |-- AvailableDtTm: string (nullable = true)\n",
      " |-- Address: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Zipcode: integer (nullable = true)\n",
      " |-- Battalion: string (nullable = true)\n",
      " |-- StationArea: integer (nullable = true)\n",
      " |-- Box: integer (nullable = true)\n",
      " |-- OriginalPriority: string (nullable = true)\n",
      " |-- Priority: string (nullable = true)\n",
      " |-- FinalPriority: integer (nullable = true)\n",
      " |-- ALSUnit: boolean (nullable = true)\n",
      " |-- CallTypeGroup: string (nullable = true)\n",
      " |-- NumAlarms: integer (nullable = true)\n",
      " |-- UnitType: string (nullable = true)\n",
      " |-- UnitSequenceInCallDispatch: integer (nullable = true)\n",
      " |-- FirePreventionDistrict: string (nullable = true)\n",
      " |-- SupervisorDistrict: integer (nullable = true)\n",
      " |-- Neighborhood: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- RowID: string (nullable = true)\n",
      " |-- Delay: double (nullable = true)\n",
      " |-- IncidentDate: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_fire_dt.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ae03b68d-af58-42a1-9dc6-2601d900a57d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------+-------------------+\n",
      "|        CallType|CallNumber|       IncidentDate|\n",
      "+----------------+----------+-------------------+\n",
      "|Medical Incident|  30010041|2003-01-01 00:00:00|\n",
      "|Medical Incident|  30010045|2003-01-01 00:00:00|\n",
      "|Medical Incident|  30010068|2003-01-01 00:00:00|\n",
      "|          Alarms|  30010080|2003-01-01 00:00:00|\n",
      "|  Structure Fire|  30010086|2003-01-01 00:00:00|\n",
      "|Medical Incident|  30010134|2003-01-01 00:00:00|\n",
      "|          Alarms|  30010135|2003-01-01 00:00:00|\n",
      "|    Vehicle Fire|  30010140|2003-01-01 00:00:00|\n",
      "|Medical Incident|  30010176|2003-01-01 00:00:00|\n",
      "|Medical Incident|  30010226|2003-01-01 00:00:00|\n",
      "|           Other|  30010240|2003-01-01 00:00:00|\n",
      "|Medical Incident|  30010310|2003-01-01 00:00:00|\n",
      "|  Structure Fire|  30010316|2003-01-01 00:00:00|\n",
      "|Medical Incident|  30010348|2003-01-01 00:00:00|\n",
      "|Medical Incident|  30010360|2003-01-01 00:00:00|\n",
      "|Medical Incident|  30010361|2003-01-01 00:00:00|\n",
      "|Medical Incident|  30010377|2003-01-01 00:00:00|\n",
      "|Medical Incident|  30010411|2003-01-01 00:00:00|\n",
      "|Medical Incident|  30010435|2003-01-01 00:00:00|\n",
      "|Medical Incident|  30010446|2003-01-01 00:00:00|\n",
      "+----------------+----------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_fire_dt.select('CallType','CallNumber', 'IncidentDate') \\\n",
    "          .where(fn.year('IncidentDate')==2003) \\\n",
    "          .alias('IncidentYear').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e31253b9-ed65-4ba0-a229-d776f04ccc40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+------------+\n",
      "|            CallType|CallNumber|IncidentYear|\n",
      "+--------------------+----------+------------+\n",
      "|    Medical Incident|   1040031|        2000|\n",
      "|Citizen Assist / ...|   1040086|        2000|\n",
      "|    Medical Incident|   1040236|        2000|\n",
      "|        Outside Fire|   1040263|        2000|\n",
      "|    Medical Incident|   1050006|        2000|\n",
      "|    Medical Incident|   1050046|        2000|\n",
      "|    Medical Incident|   1050051|        2000|\n",
      "|    Medical Incident|   1050103|        2000|\n",
      "|    Medical Incident|   1050154|        2000|\n",
      "|    Medical Incident|   1050186|        2000|\n",
      "|    Medical Incident|   1050312|        2000|\n",
      "|    Medical Incident|   1050364|        2000|\n",
      "|    Medical Incident|   1050374|        2000|\n",
      "|    Medical Incident|   1060076|        2000|\n",
      "|              Alarms|   1060094|        2000|\n",
      "|    Medical Incident|   1060128|        2000|\n",
      "|      Structure Fire|   1060140|        2000|\n",
      "|               Other|   1060165|        2000|\n",
      "|      Structure Fire|   1060230|        2000|\n",
      "|    Medical Incident|   1060246|        2000|\n",
      "+--------------------+----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_fire_dt.select('CallType','CallNumber',fn.year('IncidentDate').alias('IncidentYear')) \\\n",
    "            .where('IncidentYear=2000') \\\n",
    "            .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161b72e5-f6cf-4fce-9e5a-191230005322",
   "metadata": {},
   "source": [
    "df_fire_dt.select('CallType') \\\n",
    "          .where(col('CallType').isNotNull()) \\\n",
    "          .groupBy('CallType') \\\n",
    "          .count().show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6308a262-dbe9-4860-923d-e291c64fae72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions in the DataFrame: 8\n"
     ]
    }
   ],
   "source": [
    "### to get the number of Partitions \n",
    "\n",
    "num_partitions = df_fire.rdd.getNumPartitions()\n",
    "\n",
    "# Display the number of partitions\n",
    "print(f\"Number of partitions in the DataFrame: {num_partitions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e75a71-9b19-4dab-9932-c9279939e56e",
   "metadata": {},
   "source": [
    "#### How to choose the number of partitions in your Spark Cluster? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873880fa-dd27-47b8-a934-bba5c6fa7122",
   "metadata": {},
   "source": [
    "\n",
    "Choosing the optimal number of partitions in Spark is crucial for achieving a balance between parallelism and resource efficiency. Here's a guide to help you determine the best number of partitions:\n",
    "\n",
    "## 1. Default Behavior\n",
    "\n",
    "- **HDFS/S3 Files**: Spark usually defaults to one partition per HDFS/S3 block (typically 128 MB or 256 MB).\n",
    "- **DataFrames/RDDs**: The default number of partitions is based on the cluster's configuration, particularly `spark.default.parallelism`, which is typically set to the number of cores in the cluster.\n",
    "\n",
    "## 2. General Guidelines\n",
    "\n",
    "- **Rule of Thumb**: A common heuristic is to aim for 2-4 partitions per CPU core in your cluster. For example, if your cluster has 100 cores, you might start with 200-400 partitions.\n",
    "- **Data Size**: A typical partition size is 128 MB to 256 MB. If your dataset is 1 TB, a good starting point might be around 4000 to 8000 partitions.\n",
    "\n",
    "## 3. Workload Characteristics\n",
    "\n",
    "- **I/O Intensive Workloads**: If the workload is I/O-bound (e.g., reading/writing data), more partitions can help improve parallelism and reduce job runtime.\n",
    "- **CPU Intensive Workloads**: If the workload is CPU-bound (e.g., heavy computations), fewer partitions can help reduce overhead from task scheduling and increase the effectiveness of each task.\n",
    "\n",
    "## 4. Repartitioning\n",
    "\n",
    "- **Repartition**: If your partitions are too large, you can repartition the data to create more partitions, which is particularly useful before a shuffle operation.\n",
    "\n",
    "    ```python\n",
    "    df_repartitioned = df.repartition(num_partitions)\n",
    "    ```\n",
    "\n",
    "- **Coalesce**: If your partitions are too small, you can coalesce them to reduce the number of partitions, which is useful after a shuffle operation.\n",
    "\n",
    "    ```python\n",
    "    df_coalesced = df.coalesce(num_partitions)\n",
    "    ```\n",
    "\n",
    "## 5. Experiment and Monitor\n",
    "\n",
    "- **Experimentation**: Start with a reasonable number of partitions based on the guidelines and adjust based on the workload's performance.\n",
    "- **Monitoring**: Use Spark’s web UI to monitor the stages and tasks. Look for skewed partitions (some tasks taking significantly longer than others) and adjust accordingly.\n",
    "- **Shuffle Partitions**: For operations that involve a shuffle, like `groupBy`, `join`, or `reduceByKey`, you can control the number of shuffle partitions using:\n",
    "\n",
    "    ```python\n",
    "    spark.conf.set(\"spark.sql.shuffle.partitions\", num_partitions)\n",
    "    ```\n",
    "\n",
    "## 6. Cluster Resources\n",
    "\n",
    "Consider the available memory and CPU resources in your cluster. Too many partitions might lead to excessive task scheduling overhead, while too few might lead to inefficient use of resources and longer job execution times.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dcb57b62-90c7-49c6-a52d-770012b6b7ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+-------------+\n",
      "|CallType                       |NumberofCalls|\n",
      "+-------------------------------+-------------+\n",
      "|Medical Incident               |113794       |\n",
      "|Structure Fire                 |23319        |\n",
      "|Alarms                         |19406        |\n",
      "|Traffic Collision              |7013         |\n",
      "|Citizen Assist / Service Call  |2524         |\n",
      "|Other                          |2166         |\n",
      "|Outside Fire                   |2094         |\n",
      "|Vehicle Fire                   |854          |\n",
      "|Gas Leak (Natural and LP Gases)|764          |\n",
      "|Water Rescue                   |755          |\n",
      "|Odor (Strange / Unknown)       |490          |\n",
      "|Electrical Hazard              |482          |\n",
      "|Elevator / Escalator Rescue    |453          |\n",
      "|Smoke Investigation (Outside)  |391          |\n",
      "|Fuel Spill                     |193          |\n",
      "|HazMat                         |124          |\n",
      "|Industrial Accidents           |94           |\n",
      "|Explosion                      |89           |\n",
      "|Train / Rail Incident          |57           |\n",
      "|Aircraft Emergency             |36           |\n",
      "+-------------------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_fire_dt.select('CallType') \\\n",
    "          .where(col('CallType').isNotNull()) \\\n",
    "          .groupBy('CallType') \\\n",
    "          .agg(fn.count(col('CallType')).alias('NumberofCalls')) \\\n",
    "          .orderBy('NumberofCalls', ascending=False) \\\n",
    "          .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edd45e7-9b4f-4765-994c-a98b5041fec8",
   "metadata": {},
   "source": [
    "### Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "96393613-6f34-41a4-9457-7fb8727830b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('departuredelays.csv',header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7d955657-00d4-4e1a-90be-dd2144ab3111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01011245|    6|     602|   ABE|        ATL|\n",
      "|01020600|   -8|     369|   ABE|        DTW|\n",
      "|01021245|   -2|     602|   ABE|        ATL|\n",
      "|01020605|   -4|     602|   ABE|        ATL|\n",
      "|01031245|   -4|     602|   ABE|        ATL|\n",
      "|01030605|    0|     602|   ABE|        ATL|\n",
      "|01041243|   10|     602|   ABE|        ATL|\n",
      "|01040605|   28|     602|   ABE|        ATL|\n",
      "|01051245|   88|     602|   ABE|        ATL|\n",
      "|01050605|    9|     602|   ABE|        ATL|\n",
      "|01061215|   -6|     602|   ABE|        ATL|\n",
      "|01061725|   69|     602|   ABE|        ATL|\n",
      "|01061230|    0|     369|   ABE|        DTW|\n",
      "|01060625|   -3|     602|   ABE|        ATL|\n",
      "|01070600|    0|     369|   ABE|        DTW|\n",
      "|01071725|    0|     602|   ABE|        ATL|\n",
      "|01071230|    0|     369|   ABE|        DTW|\n",
      "|01070625|    0|     602|   ABE|        ATL|\n",
      "|01071219|    0|     569|   ABE|        ORD|\n",
      "|01080600|    0|     369|   ABE|        DTW|\n",
      "+--------+-----+--------+------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650b5af3-ea4a-4d45-8553-21b74d6d14bd",
   "metadata": {},
   "source": [
    "##### How to create a spark sql statement? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3511ac41-7cb6-40df-b63c-b094c3e6beb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('UsDelays')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bcd8c1e7-b565-4721-bbb6-1bc6982fecb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "query1= spark.sql(\"select * from UsDelays  where distance > 300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "cd03bc2d-e8de-4208-be4c-ec350a2b891a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+------+-----------+\n",
      "|   date|delay|distance|origin|destination|\n",
      "+-------+-----+--------+------+-----------+\n",
      "|1011245|    6|     602|   ABE|        ATL|\n",
      "|1020600|   -8|     369|   ABE|        DTW|\n",
      "|1021245|   -2|     602|   ABE|        ATL|\n",
      "|1020605|   -4|     602|   ABE|        ATL|\n",
      "|1031245|   -4|     602|   ABE|        ATL|\n",
      "|1030605|    0|     602|   ABE|        ATL|\n",
      "|1041243|   10|     602|   ABE|        ATL|\n",
      "|1040605|   28|     602|   ABE|        ATL|\n",
      "|1051245|   88|     602|   ABE|        ATL|\n",
      "|1050605|    9|     602|   ABE|        ATL|\n",
      "|1061215|   -6|     602|   ABE|        ATL|\n",
      "|1061725|   69|     602|   ABE|        ATL|\n",
      "|1061230|    0|     369|   ABE|        DTW|\n",
      "|1060625|   -3|     602|   ABE|        ATL|\n",
      "|1070600|    0|     369|   ABE|        DTW|\n",
      "|1071725|    0|     602|   ABE|        ATL|\n",
      "|1071230|    0|     369|   ABE|        DTW|\n",
      "|1070625|    0|     602|   ABE|        ATL|\n",
      "|1071219|    0|     569|   ABE|        ORD|\n",
      "|1080600|    0|     369|   ABE|        DTW|\n",
      "+-------+-----+--------+------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "08b39fe0-4a36-4fb7-a51c-54ee894ab711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+------+-----------+\n",
      "|   date|delay|distance|origin|destination|\n",
      "+-------+-----+--------+------+-----------+\n",
      "|1011245|    6|     602|   ABE|        ATL|\n",
      "|1020600|   -8|     369|   ABE|        DTW|\n",
      "|1021245|   -2|     602|   ABE|        ATL|\n",
      "|1020605|   -4|     602|   ABE|        ATL|\n",
      "|1031245|   -4|     602|   ABE|        ATL|\n",
      "|1030605|    0|     602|   ABE|        ATL|\n",
      "|1041243|   10|     602|   ABE|        ATL|\n",
      "|1040605|   28|     602|   ABE|        ATL|\n",
      "|1051245|   88|     602|   ABE|        ATL|\n",
      "|1050605|    9|     602|   ABE|        ATL|\n",
      "|1061215|   -6|     602|   ABE|        ATL|\n",
      "|1061725|   69|     602|   ABE|        ATL|\n",
      "|1061230|    0|     369|   ABE|        DTW|\n",
      "|1060625|   -3|     602|   ABE|        ATL|\n",
      "|1070600|    0|     369|   ABE|        DTW|\n",
      "|1071725|    0|     602|   ABE|        ATL|\n",
      "|1071230|    0|     369|   ABE|        DTW|\n",
      "|1070625|    0|     602|   ABE|        ATL|\n",
      "|1071219|    0|     569|   ABE|        ORD|\n",
      "|1080600|    0|     369|   ABE|        DTW|\n",
      "+-------+-----+--------+------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# same as query above but using Spark Dataframes\n",
    "df.select('*').where('distance > 300').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1a3e8ee7-a522-43f3-8fb2-98a4c4dd7c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "query2 = spark.sql(\"\"\" \n",
    "SELECT delay,origin, destination, \n",
    "CASE WHEN delay > 360 THEN 'Very Long Delays'\n",
    "WHEN delay > 120 AND delay < 360 THEN 'Long Delays'\n",
    "WHEN delay > 60 AND delay < 120 THEN 'Short Delays'\n",
    "WHEN delay > 0 AND delay < 60 THEN 'Tolerable Delays'\n",
    "WHEN delay = 0 THEN 'No Delays'\n",
    "ELSE 'Early'\n",
    "END AS Flights_Delays\n",
    "FROM UsDelays\n",
    "ORDER BY 2,1 Desc\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d02822fe-4f47-475e-8916-ca7c0b8bfd41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----------+--------------+\n",
      "|delay|origin|destination|Flights_Delays|\n",
      "+-----+------+-----------+--------------+\n",
      "|  333|   ABE|        ATL|   Long Delays|\n",
      "|  305|   ABE|        ATL|   Long Delays|\n",
      "|  275|   ABE|        ATL|   Long Delays|\n",
      "|  257|   ABE|        ATL|   Long Delays|\n",
      "|  247|   ABE|        ATL|   Long Delays|\n",
      "|  247|   ABE|        DTW|   Long Delays|\n",
      "|  219|   ABE|        ORD|   Long Delays|\n",
      "|  211|   ABE|        ATL|   Long Delays|\n",
      "|  197|   ABE|        DTW|   Long Delays|\n",
      "|  192|   ABE|        ORD|   Long Delays|\n",
      "|  180|   ABE|        ATL|   Long Delays|\n",
      "|  173|   ABE|        DTW|   Long Delays|\n",
      "|  165|   ABE|        ATL|   Long Delays|\n",
      "|  159|   ABE|        ORD|   Long Delays|\n",
      "|  159|   ABE|        ATL|   Long Delays|\n",
      "|  158|   ABE|        ATL|   Long Delays|\n",
      "|  151|   ABE|        DTW|   Long Delays|\n",
      "|  127|   ABE|        ATL|   Long Delays|\n",
      "|  121|   ABE|        DTW|   Long Delays|\n",
      "|  118|   ABE|        DTW|  Short Delays|\n",
      "+-----+------+-----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b723d24d-2823-48b6-923e-9d84f44343a4",
   "metadata": {},
   "source": [
    "#### Data Sources Connectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8f08a1-a916-4478-90cd-08a9b2178545",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### To connect any database with Spark you can use spark.read.jdbc or spark.read.odbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9d2ee8-0e1d-4902-a2d4-8af434825e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for PostgreSQL connection\n",
    "jdbc_url = \"jdbc:postgresql://your_host:your_port/your_database\"\n",
    "connection_properties = {\n",
    "    \"user\": \"your_username\",\n",
    "    \"password\": \"your_password\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "# Reading data from PostgreSQL table\n",
    "df = spark.read.jdbc(url=jdbc_url, table=\"your_table\", properties=connection_properties)\n",
    "\n",
    "# Show the data\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f6aa4e-36dc-4e8d-89bb-02e3838fa5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ODBC Configuration for PostgreSQL connection\n",
    "odbc_url = \"jdbc:odbc://your_host:your_port/your_database\"\n",
    "odbc_properties = {\n",
    "    \"driver\": \"org.postgresql.Driver\",\n",
    "    \"user\": \"your_username\",\n",
    "    \"password\": \"your_password\"\n",
    "}\n",
    "\n",
    "# Reading data from PostgreSQL using ODBC\n",
    "df = spark.read.format(\"jdbc\").option(\"url\", odbc_url) \\\n",
    "    .option(\"dbtable\", \"your_table\") \\\n",
    "    .option(\"user\", odbc_properties[\"user\"]) \\\n",
    "    .option(\"password\", odbc_properties[\"password\"]) \\\n",
    "    .option(\"driver\", odbc_properties[\"driver\"]) \\\n",
    "    .load()\n",
    "\n",
    "# Show the data\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddee1359-47a0-4449-93db-a81592f7f1d4",
   "metadata": {},
   "source": [
    "#### Reading Distributed Formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d0afa7b1-2d06-4248-8c99-6631a37fb009",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parquet = spark.read.parquet('userdata.parquet', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ef58287f-86f1-46fb-bc6f-7013d41ed3ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+----------+---------+----------------+------+-----------+----------------+---------+---------+--------+----------------+--------+\n",
      "|  registration_dttm| id|first_name|last_name|           email|gender| ip_address|              cc|  country|birthdate|  salary|           title|comments|\n",
      "+-------------------+---+----------+---------+----------------+------+-----------+----------------+---------+---------+--------+----------------+--------+\n",
      "|2016-02-03 07:55:29|  1|    Amanda|   Jordan|ajordan0@com.com|Female|1.197.201.2|6759521864920116|Indonesia| 3/8/1971|49756.53|Internal Auditor|   1E+02|\n",
      "+-------------------+---+----------+---------+----------------+------+-----------+----------------+---------+---------+--------+----------------+--------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_parquet.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56db2483-6534-464e-880d-5e03cfdf5efe",
   "metadata": {},
   "source": [
    "#### writing it to another format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "605f3adb-8608-45de-a79b-84dd10217d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parquet.write.csv('from_parquet_to_csv.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7716ba7-763e-4673-976c-85c69d5eebb2",
   "metadata": {},
   "source": [
    "#### Genral use to read any format is to use : spark.read.format and options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f4d20516-0da4-46b1-8c5c-edabcfd0558e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|NULL|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format('json') \\\n",
    "    .option('inferSchema','true') \\\n",
    "    .option('header','true') \\\n",
    "    .load('people.json').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850de446-0cad-4e3b-b007-4bdb5d3cda50",
   "metadata": {},
   "source": [
    "#### Another example using format for read with database connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "134efba2-5a38-46d0-81bb-9bb8cc438be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark_session():\n",
    "    \"\"\"\n",
    "    Create and configure a Spark session.\n",
    "    \"\"\"\n",
    "    spark = SparkSession.builder.config(\"spark.jars\", \"/Drivers/SQL_Sever/jdbc/postgresql-42.7.3.jar\").getOrCreate()\n",
    "    return spark\n",
    "\n",
    "def load_data_from_postgres(spark, tbl_list):\n",
    "    \"\"\"\n",
    "    Load data from PostgreSQL into Spark dataframes.\n",
    "    \n",
    "    Args:\n",
    "    - spark: The Spark session object.\n",
    "    - tbl_list: A list of table names to be loaded from PostgreSQL.\n",
    "    \n",
    "    Returns:\n",
    "    A dictionary of Spark dataframes.\n",
    "    \"\"\"\n",
    "    dataframe = {}\n",
    "    for table in tbl_list:\n",
    "        df = spark.read.format(\"jdbc\") \\\n",
    "            .option(\"url\", \"jdbc:postgresql://postgres:5432/arsenalfc\") \\\n",
    "            .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "            .option(\"dbtable\", f\"{table}\") \\\n",
    "            .option(\"user\", \"postgres\") \\\n",
    "            .option(\"password\", \"postgres\") \\\n",
    "            .load()\n",
    "        dataframe[table] = df\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41e2fdc-9de4-4602-9677-c0ecea754e80",
   "metadata": {},
   "source": [
    "#### Another example using format for writing on database "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "340ed0bb-f45b-432e-a145-d85580e6c58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_load_dim_date(spark):\n",
    "    \"\"\"\n",
    "    Create the dim_date DataFrame and load it into a PostgreSQL table.\n",
    "    \"\"\"\n",
    "    # Example logic to calculate min_date and date_diff\n",
    "    # You'll need to replace this with actual logic to determine these values\n",
    "    min_date = '2017-08-11'\n",
    "    max_date = '2023-02-25'\n",
    "    date_diff = (to_date(lit(max_date), 'yyyy-MM-dd') - to_date(lit(min_date), 'yyyy-MM-dd')).days\n",
    "\n",
    "    # Now create the date_df DataFrame\n",
    "    date_df = spark.range(date_diff + 1).select(expr(f\"date_add(to_date('{min_date}', 'yyyy-MM-dd'), id)\").alias(\"Date\"))\n",
    "    \n",
    "    # Create the dim_date DataFrame with additional date parts\n",
    "    dim_date_df = date_df.select(\n",
    "        \"Date\",\n",
    "        year(\"Date\").alias(\"Year\"),\n",
    "        month(\"Date\").alias(\"Month\"),\n",
    "        dayofmonth(\"Date\").alias(\"Day\"),\n",
    "        dayofweek(\"Date\").alias(\"Weekday\"),\n",
    "        quarter(\"Date\").alias(\"Quarter\")\n",
    "    )\n",
    "    \n",
    "    # Load the dim_date DataFrame to the PostgreSQL database\n",
    "    dim_date_df.write.format(\"jdbc\") \\\n",
    "        .option(\"url\", \"jdbc:postgresql://postgres:5432/arsenalfc\") \\\n",
    "        .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "        .option(\"dbtable\", \"dwh.DimDate\") \\\n",
    "        .option(\"user\", \"postgres\") \\\n",
    "        .option(\"password\", \"postgres\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .save()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8170de3c-49c9-4b92-8527-73038d7f2648",
   "metadata": {},
   "source": [
    "#### Thank you!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
